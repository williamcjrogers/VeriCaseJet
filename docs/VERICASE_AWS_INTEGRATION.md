# VeriCase AWS & SSH Integration Guide

## Enhancing Your VeriCase Application with AWS & SSH Capabilities

This guide shows how to integrate AWS and SSH MCP capabilities directly into your VeriCase legal case management system.

---

## üéØ Quick Wins - Immediate Enhancements

### 1. AWS Knowledge Base Integration in AI Chat

**Current State:** VeriCase uses OpenAI, Anthropic, Gemini, and Bedrock for AI chat

**Enhancement:** Add AWS Knowledge Base retrieval for context-aware responses

**Implementation:**

```python
# In vericase/api/app/ai_chat.py

from app.aws_services import get_aws_services

async def enhanced_ai_chat(question: str, case_id: str, db: Session):
    """Enhanced AI chat with AWS KB context"""
    
    # 1. Query AWS Knowledge Base for relevant context
    aws = get_aws_services()
    kb_results = await aws.query_knowledge_base(
        kb_id=settings.BEDROCK_KB_ID,
        query=question,
        max_results=5
    )
    
    # 2. Build context from KB results
    context = "\n".join([r['content'] for r in kb_results])
    
    # 3. Enhance prompt with KB context
    enhanced_prompt = f"""
    Context from legal knowledge base:
    {context}
    
    User question: {question}
    Case ID: {case_id}
    
    Provide a detailed answer using the context above.
    """
    
    # 4. Send to your preferred AI model
    response = await call_ai_model(enhanced_prompt)
    
    return {
        "answer": response,
        "sources": kb_results,
        "confidence": calculate_confidence(kb_results)
    }
```

**UI Integration:**
```javascript
// In vericase/ui/copilot.html
async function askQuestion() {
    const question = document.getElementById('question').value;
    const response = await fetch('/api/ai/chat-enhanced', {
        method: 'POST',
        body: JSON.stringify({ question, case_id: currentCaseId })
    });
    
    const data = await response.json();
    
    // Show answer with sources
    displayAnswer(data.answer);
    displaySources(data.sources); // Show KB sources
}
```

---

### 2. AWS Textract for Better OCR

**Current State:** Using basic OCR for document text extraction

**Enhancement:** Use AWS Textract for superior accuracy and table extraction

**Implementation:**

```python
# In vericase/api/app/enhanced_evidence_processor.py

async def process_document_with_textract(document_id: str, s3_key: str):
    """Process document using AWS Textract"""
    
    aws = get_aws_services()
    
    # 1. Start Textract job
    job_id = await aws.start_textract_job(
        s3_bucket=settings.S3_BUCKET,
        s3_key=s3_key,
        features=['TABLES', 'FORMS']  # Extract tables and forms too
    )
    
    # 2. Wait for completion (async)
    result = await aws.get_textract_result(job_id)
    
    # 3. Extract structured data
    extracted_text = result['text']
    tables = result['tables']
    forms = result['forms']
    
    # 4. Store in database
    document = db.get(Document, document_id)
    document.text_excerpt = extracted_text[:5000]
    document.meta = {
        **document.meta,
        'textract_job_id': job_id,
        'tables_count': len(tables),
        'forms_count': len(forms),
        'confidence': result['average_confidence']
    }
    
    # 5. Store tables separately for query
    for table in tables:
        store_document_table(document_id, table)
    
    db.commit()
    
    return {
        'status': 'success',
        'text_length': len(extracted_text),
        'tables_found': len(tables)
    }
```

**Celery Task Update:**
```python
# In vericase/worker_app/worker.py

@celery_app.task
def process_document_enhanced(doc_id: str):
    """Enhanced document processing with Textract"""
    
    # Use Textract for better extraction
    if settings.USE_TEXTRACT:
        result = process_document_with_textract(doc_id, s3_key)
    else:
        result = traditional_ocr(doc_id)
    
    # Continue with classification, indexing, etc.
    classify_document(doc_id, result['text'])
    index_in_opensearch(doc_id, result)
```

---

### 3. AWS Comprehend for Intelligent Document Classification

**Current State:** Basic keyword-based document classification

**Enhancement:** AI-powered entity extraction and classification

**Implementation:**

```python
# In vericase/api/app/ai_intelligence.py

async def classify_document_with_comprehend(text: str, document_id: str):
    """Use AWS Comprehend for intelligent classification"""
    
    aws = get_aws_services()
    
    # 1. Detect entities (people, organizations, dates, locations)
    entities = await aws.detect_entities(text)
    
    # 2. Extract key phrases
    key_phrases = await aws.detect_key_phrases(text)
    
    # 3. Classify document type
    document_type = await aws.classify_document(text)
    
    # 4. Detect PII (for redaction)
    pii = await aws.detect_pii(text)
    
    # 5. Sentiment analysis (useful for correspondence)
    sentiment = await aws.detect_sentiment(text)
    
    # 6. Store results
    return {
        'document_type': document_type,
        'entities': {
            'persons': [e for e in entities if e['Type'] == 'PERSON'],
            'organizations': [e for e in entities if e['Type'] == 'ORGANIZATION'],
            'dates': [e for e in entities if e['Type'] == 'DATE'],
            'locations': [e for e in entities if e['Type'] == 'LOCATION']
        },
        'key_phrases': key_phrases,
        'pii_detected': len(pii) > 0,
        'pii_locations': pii,
        'sentiment': sentiment
    }

# Auto-populate case fields
async def auto_populate_case_fields(case_id: str, document_id: str):
    """Auto-populate case fields from document analysis"""
    
    doc = db.get(Document, document_id)
    analysis = await classify_document_with_comprehend(doc.text_excerpt, document_id)
    
    case = db.get(Case, case_id)
    
    # Auto-extract stakeholders
    for org in analysis['entities']['organizations']:
        add_stakeholder_if_not_exists(case_id, org['Text'], 'organization')
    
    # Auto-extract key dates for timeline
    for date in analysis['entities']['dates']:
        add_timeline_event(case_id, date['Text'], f"Mentioned in {doc.filename}")
    
    # Flag sensitive documents
    if analysis['pii_detected']:
        doc.meta['contains_pii'] = True
        doc.meta['redaction_required'] = True
    
    db.commit()
```

---

### 4. Enhanced PST Analysis with AWS

**Current State:** Basic PST email extraction

**Enhancement:** AI-powered email analysis and relationship mapping

**Implementation:**

```python
# In vericase/api/app/correspondence.py

async def analyze_email_with_aws(email_id: str):
    """Enhanced email analysis using AWS services"""
    
    email = db.get(Email, email_id)
    aws = get_aws_services()
    
    # 1. Extract entities from email body
    entities = await aws.detect_entities(email.body)
    
    # 2. Classify email importance
    email_class = await classify_email_importance(email.subject + " " + email.body)
    
    # 3. Detect sentiment (useful for dispute correspondence)
    sentiment = await aws.detect_sentiment(email.body)
    
    # 4. Extract action items
    action_items = await extract_action_items(email.body)
    
    # 5. Update email metadata
    email.meta = {
        **email.meta,
        'sentiment': sentiment,
        'importance': email_class,
        'action_items': action_items,
        'mentioned_parties': [e['Text'] for e in entities if e['Type'] == 'PERSON'],
        'mentioned_dates': [e['Text'] for e in entities if e['Type'] == 'DATE']
    }
    
    # 6. Auto-create timeline events
    for date in [e for e in entities if e['Type'] == 'DATE']:
        create_timeline_event_from_email(email_id, date['Text'])
    
    db.commit()
```

**UI Enhancement:**
```javascript
// In vericase/ui/correspondence-enterprise.html
function displayEmailAnalysis(email) {
    const analysis = email.meta;
    
    // Show sentiment badge
    const sentimentBadge = `
        <span class="badge badge-${getSentimentColor(analysis.sentiment)}">
            ${analysis.sentiment}
        </span>
    `;
    
    // Show importance indicator
    const importanceIcon = analysis.importance === 'high' 
        ? '<i class="fas fa-exclamation-circle text-danger"></i>'
        : '';
    
    // Show action items
    if (analysis.action_items && analysis.action_items.length > 0) {
        displayActionItems(analysis.action_items);
    }
}
```

---

### 5. Real-Time Monitoring Dashboard

**Current State:** Basic master dashboard

**Enhancement:** AWS CloudWatch integration for real-time metrics

**Implementation:**

```python
# In vericase/api/app/dashboard_api.py

@router.get("/api/dashboard/system-health")
async def get_system_health(user: User = Depends(current_user)):
    """Get real-time system health from AWS"""
    
    aws = get_aws_services()
    
    # 1. Get EKS cluster health
    eks_health = await aws.get_eks_cluster_health()
    
    # 2. Get RDS database metrics
    db_metrics = await aws.get_rds_metrics(
        db_instance='vericase-prod',
        metrics=['CPUUtilization', 'DatabaseConnections', 'FreeableMemory']
    )
    
    # 3. Get S3 storage usage
    s3_usage = await aws.get_s3_bucket_size(settings.S3_BUCKET)
    
    # 4. Get recent errors from CloudWatch
    recent_errors = await aws.get_cloudwatch_logs(
        log_group='/aws/eks/vericase/api',
        filter_pattern='ERROR',
        hours=1
    )
    
    # 5. Get Celery queue length
    redis_info = await get_redis_info()
    
    return {
        'timestamp': datetime.now(),
        'eks': {
            'status': eks_health['status'],
            'node_count': eks_health['node_count'],
            'pod_count': eks_health['pod_count']
        },
        'database': {
            'cpu': db_metrics['CPUUtilization'],
            'connections': db_metrics['DatabaseConnections'],
            'available_memory': db_metrics['FreeableMemory']
        },
        'storage': {
            'size_gb': s3_usage / (1024**3),
            'document_count': get_document_count()
        },
        'processing': {
            'queue_length': redis_info['queue_length'],
            'active_workers': redis_info['active_workers']
        },
        'errors': {
            'count': len(recent_errors),
            'recent': recent_errors[:5]
        }
    }
```

**UI Implementation:**
```javascript
// In vericase/ui/master-dashboard.html
async function updateSystemHealth() {
    const response = await fetch('/api/dashboard/system-health');
    const health = await response.json();
    
    // Update health indicators
    document.getElementById('eks-status').textContent = health.eks.status;
    document.getElementById('db-cpu').textContent = `${health.database.cpu}%`;
    document.getElementById('storage-size').textContent = `${health.storage.size_gb.toFixed(2)} GB`;
    document.getElementById('queue-length').textContent = health.processing.queue_length;
    
    // Show alerts if needed
    if (health.database.cpu > 80) {
        showAlert('High database CPU usage detected');
    }
    
    if (health.errors.count > 10) {
        showAlert(`${health.errors.count} errors in past hour`);
    }
}

// Refresh every 30 seconds
setInterval(updateSystemHealth, 30000);
```

---

### 6. SSH-Based Deployment Tools

**Current State:** Manual deployments

**Enhancement:** Built-in deployment management

**Implementation:**

```python
# In vericase/api/app/deployment_tools.py (new file)

from fastapi import APIRouter
import asyncio
import asyncssh

router = APIRouter()

@router.post("/api/admin/deploy/{environment}")
async def deploy_to_environment(
    environment: str,
    user: User = Depends(current_user)
):
    """Deploy to staging or production via SSH"""
    
    # Only admins can deploy
    if user.role != UserRole.ADMIN:
        raise HTTPException(403, "Admin access required")
    
    if environment not in ['staging', 'production']:
        raise HTTPException(400, "Invalid environment")
    
    # Get SSH connection details
    if environment == 'staging':
        host = settings.STAGING_HOST
        key_path = settings.STAGING_KEY_PATH
    else:
        host = settings.PRODUCTION_HOST
        key_path = settings.PRODUCTION_KEY_PATH
    
    deployment_log = []
    
    async with asyncssh.connect(host, username='ubuntu', client_keys=[key_path]) as conn:
        # 1. Backup database
        deployment_log.append("Creating database backup...")
        result = await conn.run('pg_dump vericase > /tmp/backup.sql')
        deployment_log.append(f"Backup: {result.stdout}")
        
        # 2. Pull latest code
        deployment_log.append("Pulling latest code...")
        result = await conn.run('cd /opt/vericase && git pull origin main')
        deployment_log.append(f"Git pull: {result.stdout}")
        
        # 3. Build Docker images
        deployment_log.append("Building Docker images...")
        result = await conn.run('cd /opt/vericase && docker-compose build')
        deployment_log.append(f"Build: {result.stdout}")
        
        # 4. Run migrations
        deployment_log.append("Running database migrations...")
        result = await conn.run('cd /opt/vericase && docker-compose run api alembic upgrade head')
        deployment_log.append(f"Migrations: {result.stdout}")
        
        # 5. Restart services
        deployment_log.append("Restarting services...")
        result = await conn.run('cd /opt/vericase && docker-compose up -d')
        deployment_log.append(f"Restart: {result.stdout}")
        
        # 6. Health check
        deployment_log.append("Running health check...")
        await asyncio.sleep(10)  # Wait for services to start
        result = await conn.run('curl -f http://localhost:8000/health')
        
        if result.exit_status == 0:
            deployment_log.append("‚úÖ Deployment successful!")
            status = "success"
        else:
            deployment_log.append("‚ùå Health check failed - rolling back")
            await conn.run('cd /opt/vericase && git reset --hard HEAD~1 && docker-compose up -d')
            status = "failed"
    
    return {
        "environment": environment,
        "status": status,
        "log": deployment_log,
        "timestamp": datetime.now()
    }

@router.get("/api/admin/server-status")
async def get_server_status(user: User = Depends(current_user)):
    """Get status of all servers via SSH"""
    
    if user.role != UserRole.ADMIN:
        raise HTTPException(403)
    
    servers = {}
    
    for env in ['staging', 'production']:
        host = settings.STAGING_HOST if env == 'staging' else settings.PRODUCTION_HOST
        key_path = settings.STAGING_KEY_PATH if env == 'staging' else settings.PRODUCTION_KEY_PATH
        
        async with asyncssh.connect(host, username='ubuntu', client_keys=[key_path]) as conn:
            # Get system info
            uptime = await conn.run('uptime')
            disk = await conn.run('df -h / | tail -1')
            docker_ps = await conn.run('docker ps --format "{{.Names}}: {{.Status}}"')
            
            servers[env] = {
                "uptime": uptime.stdout.strip(),
                "disk_usage": disk.stdout.strip(),
                "containers": docker_ps.stdout.strip().split('\n')
            }
    
    return servers
```

**UI for Deployment:**
```html
<!-- In vericase/ui/admin-deploy.html (new file) -->
<div class="deployment-panel">
    <h3>Deployment Management</h3>
    
    <div class="environment-selector">
        <button onclick="deploy('staging')" class="btn btn-warning">
            Deploy to Staging
        </button>
        <button onclick="deploy('production')" class="btn btn-danger">
            Deploy to Production
        </button>
    </div>
    
    <div id="deployment-log" class="log-viewer"></div>
    
    <div class="server-status">
        <h4>Server Status</h4>
        <div id="server-status-container"></div>
    </div>
</div>

<script>
async function deploy(environment) {
    if (!confirm(`Deploy to ${environment}? This will restart services.`)) return;
    
    const logViewer = document.getElementById('deployment-log');
    logViewer.innerHTML = '<div class="spinner">Deploying...</div>';
    
    const response = await fetch(`/api/admin/deploy/${environment}`, {
        method: 'POST',
        headers: { 'Authorization': `Bearer ${getToken()}` }
    });
    
    const result = await response.json();
    
    // Display log
    logViewer.innerHTML = result.log.map(line => 
        `<div class="log-line">${line}</div>`
    ).join('');
    
    // Show result
    if (result.status === 'success') {
        showSuccess(`Deployed to ${environment} successfully!`);
    } else {
        showError(`Deployment to ${environment} failed. See log for details.`);
    }
}

async function refreshServerStatus() {
    const response = await fetch('/api/admin/server-status');
    const status = await response.json();
    
    const container = document.getElementById('server-status-container');
    container.innerHTML = Object.entries(status).map(([env, info]) => `
        <div class="server-card">
            <h5>${env}</h5>
            <p><strong>Uptime:</strong> ${info.uptime}</p>
            <p><strong>Disk:</strong> ${info.disk_usage}</p>
            <p><strong>Containers:</strong></p>
            <ul>
                ${info.containers.map(c => `<li>${c}</li>`).join('')}
            </ul>
        </div>
    `).join('');
}

setInterval(refreshServerStatus, 60000); // Refresh every minute
</script>
```

---

### 7. Evidence Repository Enhancement

**Current State:** Basic evidence storage

**Enhancement:** AI-powered evidence analysis and linking

**Implementation:**

```python
# In vericase/api/app/evidence_repository.py

@router.post("/api/evidence/analyze/{evidence_id}")
async def analyze_evidence_with_aws(
    evidence_id: str,
    db: Session = Depends(get_db),
    user: User = Depends(current_user)
):
    """Comprehensive evidence analysis using AWS"""
    
    evidence = db.get(EvidenceItem, evidence_id)
    aws = get_aws_services()
    
    # 1. Get full text content
    if evidence.s3_key:
        # Download from S3
        content = await aws.get_object(settings.S3_BUCKET, evidence.s3_key)
        
        # If PDF, extract with Textract
        if evidence.file_extension == '.pdf':
            text = await aws.extract_text_textract(settings.S3_BUCKET, evidence.s3_key)
        else:
            text = content.decode('utf-8')
    else:
        text = evidence.content
    
    # 2. Comprehensive analysis
    entities = await aws.detect_entities(text)
    key_phrases = await aws.detect_key_phrases(text)
    pii = await aws.detect_pii(text)
    
    # 3. Find related evidence
    # Query KB for similar documents
    similar = await aws.query_knowledge_base(
        kb_id=settings.BEDROCK_KB_ID,
        query=text[:500],  # Use first 500 chars as query
        max_results=5
    )
    
    # 4. Auto-generate summary
    summary = await generate_evidence_summary(text)
    
    # 5. Update evidence record
    evidence.meta = {
        **evidence.meta,
        'analysis': {
            'entities': entities,
            'key_phrases': key_phrases,
            'pii_detected': len(pii) > 0,
            'summary': summary,
            'related_evidence': [s['id'] for s in similar]
        },
        'analyzed_at': datetime.now().isoformat()
    }
    
    db.commit()
    
    return {
        'evidence_id': evidence_id,
        'analysis': evidence.meta['analysis'],
        'status': 'completed'
    }

async def generate_evidence_summary(text: str) -> str:
    """Generate AI summary of evidence"""
    
    prompt = f"""
    Summarize this legal evidence in 2-3 sentences, focusing on:
    - Key facts
    - Relevant dates
    - Important parties mentioned
    - Significance to the case
    
    Evidence text:
    {text[:2000]}
    """
    
    # Use your preferred AI model
    response = await call_bedrock_model(prompt)
    return response
```

---

### 8. Slack Integration for Case Updates

**Enhancement:** Real-time notifications for important events

**Implementation:**

```python
# In vericase/api/app/notifications.py (new file)

from app.integrations.slack import send_slack_notification

async def notify_important_event(event_type: str, details: dict):
    """Send notifications for important case events"""
    
    if event_type == 'high_value_evidence':
        message = f"""
        üîç *Important Evidence Detected*
        Case: {details['case_name']}
        Evidence: {details['evidence_name']}
        Importance: {details['importance_score']}
        
        <{details['evidence_url']}|View Evidence>
        """
        await send_slack_notification('#case-updates', message)
    
    elif event_type == 'deadline_approaching':
        message = f"""
        ‚è∞ *Deadline Approaching*
        Case: {details['case_name']}
        Deadline: {details['deadline_date']}
        Days remaining: {details['days_remaining']}
        
        <{details['case_url']}|View Case>
        """
        await send_slack_notification('#deadlines', message)
    
    elif event_type == 'deployment_complete':
        message = f"""
        ‚úÖ *Deployment Completed*
        Environment: {details['environment']}
        Version: {details['version']}
        Status: {details['status']}
        """
        await send_slack_notification('#deployments', message)

# Integrate into existing workflows
@router.post("/uploads/complete")
async def complete_upload_enhanced(...):
    # ... existing upload logic ...
    
    # Analyze uploaded document
    if is_important_document(doc):
        await notify_important_event('high_value_evidence', {
            'case_name': case.name,
            'evidence_name': doc.filename,
            'importance_score': doc.meta.get('importance'),
            'evidence_url': f"{settings.BASE_URL}/evidence/{doc.id}"
        })
```

---

## üöÄ Implementation Priority

### Phase 1: Immediate Value (Week 1)
1. ‚úÖ AWS Knowledge Base in AI chat
2. ‚úÖ AWS Textract for OCR
3. ‚úÖ Real-time monitoring dashboard

### Phase 2: Enhanced Intelligence (Week 2-3)
4. ‚úÖ AWS Comprehend for document classification
5. ‚úÖ Evidence analysis automation
6. ‚úÖ Email sentiment analysis

### Phase 3: Operations & Automation (Week 4)
7. ‚úÖ SSH deployment tools
8. ‚úÖ Slack notifications
9. ‚úÖ Automated backups

---

## üìã Configuration Required

Add to `vericase/api/app/config.py`:

```python
class Settings(BaseSettings):
    # ... existing settings ...
    
    # AWS Enhanced Features
    USE_TEXTRACT: bool = True
    USE_COMPREHEND: bool = True
    USE_KNOWLEDGE_BASE: bool = True
    BEDROCK_KB_ID: str = "ACKHIYIHPK"
    
    # Deployment Settings
    STAGING_HOST: str = ""
    STAGING_KEY_PATH: str = ""
    PRODUCTION_HOST: str = ""
    PRODUCTION_KEY_PATH: str = ""
    
    # Slack Integration
    SLACK_WEBHOOK_URL: str = ""
    SLACK_BOT_TOKEN: str = ""
```

---

## üíª Next Steps

1. **Reload VS Code** to activate MCP servers
2. **Test AWS connection**: `"List my S3 buckets"`
3. **Review integration points** in your code
4. **Start with Phase 1** implementations
5. **Monitor costs** in AWS Cost Explorer

---

## üéØ Expected Benefits

- **50% faster document processing** with Textract
- **80% better entity extraction** with Comprehend
- **Real-time production monitoring** with CloudWatch
- **Zero-downtime deployments** with SSH automation
- **Instant legal research** with KB integration
- **Proactive alerts** for important events

---

## üìö Related Documentation

- [MCP Quick Start](../MCP_QUICKSTART.md)
- [MCP Setup Guide](./MCP_AWS_SSH_SETUP.md)
- [MCP Enhancement Guide](./MCP_ENHANCEMENT_GUIDE.md)
- [VeriCase AI Configuration](./AI_CONFIGURATION_GUIDE.md)
